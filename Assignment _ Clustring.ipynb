{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL+1x66rzBYcrQGIj4FTwP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Theory\n","\n","1. What is unsupervised learning in the context of machine learning?\n","\n","ans- Unsupervised learning is a type of machine learning where the algorithm learns patterns and structures from data that does not have labeled responses or target variables. Unlike supervised learning, there's no \"teacher\" providing correct answers. The goal is to discover inherent groupings, relationships, or distributions within the data itself. Common tasks include clustering, dimensionality reduction, and anomaly detection.\n","\n","2. How does K-Means clustering algorithm work?\n","\n","ans- K-Means clustering is an iterative algorithm that aims to partition n observations into k clusters, where each observation belongs to the cluster with the nearest mean (centroid). Here's how it generally works:\n","\n","Initialization:\n","\n","Choose the number of clusters, k.\n","Randomly select k data points from the dataset as initial centroids.\n","Assignment Step (E-step - Expectation):\n","\n","For each data point, calculate its distance (usually Euclidean distance) to all k centroids.\n","Assign each data point to the cluster whose centroid is closest.\n","Update Step (M-step - Maximization):\n","\n","Recalculate the centroids for each cluster by taking the mean (average) of all data points assigned to that cluster. The centroid shifts to the center of its assigned points.\n","Iteration:\n","\n","Repeat steps 2 and 3 until the cluster assignments no longer change, or the centroids no longer move significantly (i.e., convergence). This indicates that the algorithm has found a stable partitioning.\n","\n","3. Explain the concept of a dendrogram in hierarchical clustering.\n","\n","ans- A dendrogram is a tree-like diagram that visually represents the steps taken in hierarchical clustering. It shows the hierarchical relationships between clusters or individual data points.\n","\n","X-axis: Represents the individual data points (leaves of the tree) or the clusters.\n","Y-axis: Represents the dissimilarity (or distance) at which clusters are merged (in agglomerative clustering) or split (in divisive clustering). The height of each \"V\" shape in the dendrogram indicates the distance between the clusters being joined/split.\n","\n","4. What is the main difference between K-Means and Hierarchical Clustering?\n","\n","ans- ![Screenshot 2025-05-27 174309.png](data:image/png;base64,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)\n","\n","5. What are the advantages of DBSCAN over K-Means?\n","\n","ans - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several advantages over K-Means:\n","\n","Does not require specifying the number of clusters (k) beforehand: DBSCAN discovers the number of clusters automatically based on data density.\n","Can find arbitrarily shaped clusters: K-Means assumes spherical (convex) clusters, whereas DBSCAN can identify clusters of complex, non-linear shapes.\n","Identifies noise points (outliers): DBSCAN explicitly designates points that do not belong to any cluster as noise, which K-Means forces into a cluster.\n","Robust to outliers: The presence of outliers does not significantly affect the formation of dense clusters.\n","\n","6. When would you use Silhouette Score in clustering?\n","\n","ans- You would use the Silhouette Score as an evaluation metric in clustering when you want to:\n","\n","Assess the quality of your clustering: It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). A high silhouette score indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters.\n","Determine the optimal number of clusters (k): You can run a clustering algorithm (like K-Means) for various values of k and calculate the Silhouette Score for each. The k that yields the highest Silhouette Score is often considered the optimal number of clusters.\n","Compare different clustering algorithms: You can use it to compare the performance of different algorithms on the same dataset.\n","\n","7. What are the limitations of Hierarchical Clustering?\n","\n","ans - Despite its advantages, Hierarchical Clustering has limitations:\n","\n","Computational Complexity: For a dataset with N data points, the time complexity is typically O(N\n","3\n"," ) (for agglomerative with standard linkage) and space complexity is O(N\n","2\n"," ) to store the proximity matrix. This makes it computationally expensive and slow for very large datasets.\n","Scalability Issues: Due to its complexity, it doesn't scale well to large datasets.\n","Irrevocable Decisions: Once a merge (agglomerative) or split (divisive) is made, it cannot be undone. This means that if a bad decision is made early on, it can negatively impact the final cluster structure.\n","Sensitivity to Noise and Outliers: Individual outliers can significantly affect the dendrogram structure, especially with certain linkage methods.\n","Difficulty with Large Differences in Cluster Densities: May struggle to find clusters of varying densities.\n","No Objective Function: Unlike K-Means, there's no direct objective function to minimize, making it harder to quantitatively assess the \"goodness\" of the clustering without external metrics.\n","\n","8. Why is feature scaling important in clustering algorithms like K-Means?\n","\n","ans - Feature scaling is crucial in K-Means (and many other distance-based algorithms) because:\n","\n","Distance-based: K-Means relies on distance calculations (e.g., Euclidean distance) to determine cluster assignments. If features have vastly different scales, features with larger ranges will dominate the distance calculations.\n","Example: If 'Age' ranges from 0-100 and 'Income' ranges from 10,000-100,000, the 'Income' feature will disproportionately influence the distance, making 'Age' almost irrelevant, even if it's a very important differentiating factor.\n","Fair Contribution: Scaling ensures that all features contribute equally to the distance computation, allowing the algorithm to properly identify clusters based on the actual relationships between data points across all dimensions.\n","Improved Convergence: Scaling can lead to faster convergence of the K-Means algorithm by preventing numerical instability that might arise from large differences in feature magnitudes.\n","\n","9. How does DBSCAN identify noise points?\n","\n","ans- DBSCAN identifies noise points based on its density-reachability concept:\n","\n","Core Point: A point is a core point if it has at least min_samples (a parameter) neighbors within a distance of eps (another parameter).\n","Border Point: A point is a border point if it has fewer than min_samples neighbors within eps, but it is within eps distance of a core point.\n","Noise Point (Outlier): A point is considered a noise point if it is neither a core point nor a border point. This means it has fewer than min_samples neighbors within its eps radius, and it's not density-reachable from any core point. These are isolated points in low-density regions.\n","\n","10. Define inertia in the context of K-Means.\n","\n","ans- Inertia, also known as the Within-Cluster Sum of Squares (WCSS), is a metric used to evaluate the quality of a K-Means clustering.\n","\n","Definition: It is the sum of squared distances of samples to their closest cluster center (centroid).\n","Goal: K-Means algorithm tries to minimize inertia. A lower inertia value generally indicates better clustering, meaning data points are closer to their respective cluster centroids.\n","Formula:\n","$ \\text{Inertia} = \\sum_{j=1}^{k} \\sum_{x \\in C_j} ||x - \\mu_j||^2 $\n","where:\n","\n","k is the number of clusters.\n","C\n","j\n","​\n","  is the set of points in cluster j.\n","μ\n","j\n","​\n","  is the centroid of cluster j.\n","∣∣x−μ\n","j\n","​\n"," ∣∣\n","2\n","  is the squared Euclidean distance between point x and centroid μ\n","j\n","​\n"," .\n","\n"," 11. What is the elbow method in K-Means clustering?\n","\n","ans- The elbow method is a heuristic used to estimate the optimal number of clusters (k) for K-Means.\n","\n","How it works:\n","\n","Run the K-Means algorithm for a range of k values (e.g., from 1 to 10 or 15).\n","For each value of k, calculate the inertia (WCSS).\n","Plot the inertia values against the corresponding k values.\n","\n","12. Describe the concept of \"density\" in DBSCAN.\n","\n","ans- In DBSCAN, \"density\" is defined based on two key parameters:\n","\n","eps (epsilon): This is the maximum radius of the neighborhood around a data point. It defines how close points must be to each other to be considered part of the same cluster.\n","min_samples: This is the minimum number of data points required to form a dense region (a cluster).\n","\n","13. Can hierarchical clustering be used on categorical data?\n","\n","ans- Yes, hierarchical clustering can be used on categorical data, but it requires appropriate pre-processing and distance metrics.\n","\n","Distance Metrics: You cannot directly use Euclidean distance for categorical features. You need to use distance metrics suitable for categorical data, such as:\n","Hamming distance: For binary categorical data (number of differing positions).\n","Gower distance: A more general dissimilarity measure that can handle mixed data types (numerical and categorical). For categorical features, it typically assigns 0 if categories match and 1 if they don't.\n","Jaccard distance: For binary attributes, often used in market basket analysis.\n","Encoding: Categorical features might need to be encoded, but simple one-hot encoding can lead to high dimensionality and sparse data, which might not be ideal for distance calculations without careful consideration. Using appropriate distance metrics or specialized encoding (e.g., using frequency or target encoding if applicable, but be cautious of information leakage if used in supervised contexts) is more robust.\n","\n","14. What does a negative Silhouette Score indicate?\n","\n","ans- A negative Silhouette Score indicates that the sample might have been assigned to the wrong cluster, or that it is closer to a neighboring cluster than to its own cluster.\n","\n","Specifically:\n","\n","A point i has a negative silhouette score if its average dissimilarity to points in its own cluster (a(i)) is greater than its average dissimilarity to points in the nearest neighboring cluster (b(i)).\n","This means the point is a poor fit for its assigned cluster and might be better suited to another cluster.\n","\n","15. Explain the term \"linkage criteria\" in hierarchical clustering.\n","\n","ans- In agglomerative hierarchical clustering, \"linkage criteria\" (or linkage method) defines how the distance between two clusters is calculated when they are being merged. It determines which clusters are combined at each step of the algorithm.\n","\n","Common linkage criteria include:\n","\n","Single Linkage (MIN): The distance between two clusters is the minimum distance between any single data point in the first cluster and any single data point in the second cluster. (Prone to \"chaining\" effect).\n","Complete Linkage (MAX): The distance between two clusters is the maximum distance between any single data point in the first cluster and any single data point in the second cluster. (Favors compact, spherical clusters).\n","Average Linkage: The distance between two clusters is the average distance between all pairs of data points from the two clusters. (A compromise between single and complete linkage).\n","Centroid Linkage: The distance between two clusters is the distance between their respective centroids.\n","Ward's Method: Calculates the increase in the within-cluster sum of squares (WCSS) that results from merging two clusters. It aims to minimize the variance within each cluster. (Often used for general-purpose clustering and works well with Euclidean distances).\n","\n","16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n","\n","ans- K-Means assumes that clusters are:\n","\n","Spherical (or convex): It tries to find clusters that are roughly circular or oval-shaped around a central point.\n","Of similar size (diameter): It divides the data space into Voronoi cells, which can lead to larger clusters being split or smaller clusters being absorbed if they are not of comparable size.\n","Of similar density: K-Means tries to minimize the sum of squared distances. If one cluster is very dense and another is sparse, the sparse cluster might be disproportionately expanded or split to accommodate the \"average\" distance, while the dense cluster might be inappropriately split.\n","\n","17. What are the core parameters in DBSCAN, and how do they influence clustering?\n","\n","ans- The two core parameters in DBSCAN are:\n","\n","eps (epsilon):\n","\n","Definition: The maximum radius of the neighborhood around a data point.\n","Influence:\n","Larger eps: Leads to larger neighborhoods. More points will be considered neighbors, potentially merging distinct clusters that are close together and reducing the number of noise points.\n","Smaller eps: Leads to smaller neighborhoods. Fewer points will be considered neighbors, potentially splitting single clusters into multiple smaller ones or increasing the number of noise points.\n","Too small eps: All points might become noise.\n","Too large eps: All points might form a single large cluster.\n","min_samples:\n","\n","Definition: The minimum number of data points (including the point itself) required within an eps radius to form a dense region (i.e., for a point to be considered a core point).\n","Influence:\n","Larger min_samples: Requires denser regions to form clusters. This will lead to fewer, potentially larger, and more robust clusters, while increasing the number of noise points. It makes the algorithm more conservative.\n","Smaller min_samples: Makes it easier for points to become core points, potentially leading to more numerous, smaller clusters, and reducing the number of noise points (even in sparse regions).\n","\n","18. How does K-Means++ improve upon standard K-Means initialization?\n","\n","ans- Standard K-Means initializes centroids randomly, which can lead to:\n","\n","Poor convergence: If initial centroids are poorly chosen (e.g., all in one cluster), the algorithm might take many iterations to converge.\n","Suboptimal solutions (local optima): The algorithm might get stuck in a local minimum, resulting in a suboptimal clustering.\n","K-Means++ is an intelligent initialization algorithm that addresses these issues by selecting initial centroids that are generally \"far apart\" from each other. Here's how it works:\n","\n","Select the first centroid: Randomly choose one data point from the dataset as the first centroid.\n","Calculate squared distances: For each remaining data point x, calculate its squared distance D(x)\n","2\n","  to the closest centroid that has already been chosen.\n","Select the next centroid: Choose the next centroid from the remaining data points with a probability proportional to D(x)\n","2\n"," . This means points that are farther away from existing centroids have a higher chance of being selected.\n","Repeat: Repeat steps 2 and 3 until k centroids have been selected.\n","Benefits of K-Means++:\n","\n","Faster Convergence: By starting with well-separated centroids, the algorithm typically converges faster to a stable solution.\n","Better Quality Clusters: It is much more likely to find a globally optimal or near-optimal solution, avoiding many local optima compared to purely random initialization.\n","Reduced Sensitivity to Initialization: The results are more consistent across different runs.\n","\n","19. What is agglomerative clustering?\n","\n","ans- Agglomerative clustering is a \"bottom-up\" approach to hierarchical clustering. It starts by treating each individual data point as its own cluster. Then, it iteratively merges the two closest clusters until all data points are part of a single large cluster, or until a predefined number of clusters is reached.\n","\n","Steps:\n","\n","Initialization: Each data point is considered a single cluster.\n","Calculate Proximity Matrix: Compute the pairwise distance (or dissimilarity) between all clusters (initially, between all individual points).\n","Merge Clusters: Find the two closest clusters (based on a chosen \"linkage criteria\" - see Q15) and merge them into a new, single cluster.\n","Update Proximity Matrix: Recompute the distances between the new cluster and all other existing clusters.\n","Repeat: Repeat steps 3 and 4 until all data points are merged into a single cluster or the desired number of clusters is achieved.\n","\n","20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n","\n","ans- While inertia is useful, the Silhouette Score offers several advantages that make it a better metric for evaluating clustering models, especially for determining the optimal number of clusters:\n","\n","Addresses the \"Always Decreasing\" Problem of Inertia:\n","\n","Inertia: Always decreases or stays the same as you increase the number of clusters (k). This means inertia alone will always suggest that more clusters are better, even if they're not meaningful.\n","Silhouette Score: It has a clear optimal point. A higher Silhouette Score indicates better clustering, and it will often peak at a reasonable k value and then decrease, providing a more reliable indicator for the \"best\" k.\n","Considers Both Cohesion and Separation:\n","\n","Inertia: Only measures the cohesion of points within their own cluster (how tightly packed they are around their centroid). It doesn't consider how well separated clusters are from each other.\n","Silhouette Score: Measures both cohesion (average distance to points in its own cluster, a(i)) and separation (average distance to points in the nearest neighboring cluster, b(i)). It essentially calculates how well a point fits its own cluster and how poorly it fits neighboring clusters.\n","Provides Intuitive Interpretation:\n","\n","The range of -1 to +1 for the Silhouette Score provides a clear and interpretable scale:\n","+1: Excellent clustering (point is far from neighboring clusters).\n","0: Ambiguous assignment (point is on or near the boundary).\n","-1: Incorrect assignment (point is closer to a different cluster)."],"metadata":{"id":"kqP2VbYHp7_h"}},{"cell_type":"markdown","source":["#Practical\n","\n"],"metadata":{"id":"0QvQqyxurbFh"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.datasets import make_blobs, make_moons, make_circles, load_wine, load_iris, load_digits\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n","from sklearn.metrics import silhouette_score, confusion_matrix\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","# Suppress warnings for cleaner output\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set a random seed for reproducibility\n","np.random.seed(42)"],"metadata":{"id":"lXZrNVJsr_Ia"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot."],"metadata":{"id":"5eqvEkI4r_49"}},{"cell_type":"code","source":["# 1. Generate synthetic data\n","X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.8, random_state=42)\n","\n","# 2. Apply K-Means clustering\n","kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # n_init for robustness\n","kmeans.fit(X)\n","y_kmeans = kmeans.predict(X)\n","centers = kmeans.cluster_centers_\n","\n","# 3. Visualize using a scatter plot\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.7)\n","plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8, marker='X', label='Centroids')\n","plt.title('K-Means Clustering on make_blobs (4 Centers)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.colorbar(label='Cluster ID')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"K_1Iy16BsJ0Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels."],"metadata":{"id":"lYUyJx3SsML3"}},{"cell_type":"code","source":["print(\"--- Task 22: Agglomerative Clustering on Iris, show first 10 labels ---\")\n","# Load Iris dataset\n","iris = load_iris()\n","X_iris, y_iris_true = iris.data, iris.target # y_iris_true is for verification/comparison, not used by clustering\n","\n","# Apply Agglomerative Clustering\n","n_clusters_iris = 3\n","agg_clustering = AgglomerativeClustering(n_clusters=n_clusters_iris, linkage='ward') # 'ward' is common for general use\n","labels_agg = agg_clustering.fit_predict(X_iris)\n","\n","print(f\"First 10 predicted labels for Iris dataset: {labels_agg[:10]}\")\n","print(\"-\" * 50)"],"metadata":{"id":"HsjHL5Bwsp8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot."],"metadata":{"id":"PKfd65txsrLR"}},{"cell_type":"code","source":["print(\"--- Task 23: DBSCAN on make_moons, highlight outliers ---\")\n","# Generate synthetic data with make_moons\n","n_samples_moons = 500\n","X_moons, y_moons = make_moons(n_samples=n_samples_moons, noise=0.05, random_state=42)\n","\n","# Apply DBSCAN\n","# You might need to tune eps and min_samples for optimal results\n","eps_moons = 0.1\n","min_samples_moons = 5\n","dbscan_moons = DBSCAN(eps=eps_moons, min_samples=min_samples_moons)\n","labels_dbscan_moons = dbscan_moons.fit_predict(X_moons)\n","\n","# -1 in DBSCAN labels indicates noise points (outliers)\n","n_clusters_moons = len(set(labels_dbscan_moons)) - (1 if -1 in labels_dbscan_moons else 0)\n","n_noise_points = np.sum(labels_dbscan_moons == -1)\n","\n","print(f\"DBSCAN found {n_clusters_moons} clusters and {n_noise_points} noise points.\")\n","\n","# Visualize the clusters and outliers\n","plt.figure(figsize=(8, 6))\n","unique_labels = set(labels_dbscan_moons)\n","colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n","for k, col in zip(unique_labels, colors):\n","    if k == -1:\n","        # Black used for noise.\n","        col = [0, 0, 0, 1]\n","\n","    class_member_mask = (labels_dbscan_moons == k)\n","    xy = X_moons[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","\n","plt.title(f'DBSCAN Clustering on make_moons (eps={eps_moons}, min_samples={min_samples_moons})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"IQU9gBCdssex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster."],"metadata":{"id":"yzBxPxUuster"}},{"cell_type":"code","source":["print(\"--- Task 24: K-Means on Wine with Standardization, print cluster sizes ---\")\n","# Load Wine dataset\n","wine = load_wine()\n","X_wine, y_wine = wine.data, wine.target\n","\n","# Standardize the features\n","scaler_wine = StandardScaler()\n","X_wine_scaled = scaler_wine.fit_transform(X_wine)\n","\n","# Apply K-Means clustering (Wine dataset typically has 3 classes)\n","n_clusters_wine = 3\n","kmeans_wine = KMeans(n_clusters=n_clusters_wine, random_state=42, n_init=10)\n","kmeans_wine.fit(X_wine_scaled)\n","labels_kmeans_wine = kmeans_wine.labels_\n","\n","# Print the size of each cluster\n","cluster_sizes = pd.Series(labels_kmeans_wine).value_counts().sort_index()\n","print(f\"Cluster sizes for K-Means on Wine dataset (after standardization):\\n{cluster_sizes}\")\n","\n","# Optional: You can compare these to the true class sizes:\n","# print(\"\\nTrue class sizes for Wine dataset:\")\n","# print(pd.Series(y_wine).value_counts().sort_index())\n","print(\"-\" * 50)"],"metadata":{"id":"wwZUzE7Isut8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result."],"metadata":{"id":"Y2lk2oqvsvln"}},{"cell_type":"code","source":["print(\"--- Task 25: DBSCAN on make_circles and Plot ---\")\n","# Generate synthetic data with make_circles\n","n_samples_circles = 500\n","X_circles, y_circles = make_circles(n_samples=n_samples_circles, noise=0.05, factor=0.5, random_state=42)\n","\n","# Apply DBSCAN\n","# Tuning eps and min_samples is crucial for make_circles\n","eps_circles = 0.1\n","min_samples_circles = 5\n","dbscan_circles = DBSCAN(eps=eps_circles, min_samples=min_samples_circles)\n","labels_dbscan_circles = dbscan_circles.fit_predict(X_circles)\n","\n","n_clusters_circles = len(set(labels_dbscan_circles)) - (1 if -1 in labels_dbscan_circles else 0)\n","n_noise_points_circles = np.sum(labels_dbscan_circles == -1)\n","\n","print(f\"DBSCAN found {n_clusters_circles} clusters and {n_noise_points_circles} noise points.\")\n","\n","# Visualize the clusters and outliers\n","plt.figure(figsize=(8, 6))\n","unique_labels_circles = set(labels_dbscan_circles)\n","colors_circles = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_circles))]\n","for k, col in zip(unique_labels_circles, colors_circles):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","\n","    class_member_mask = (labels_dbscan_circles == k)\n","    xy = X_circles[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","\n","plt.title(f'DBSCAN Clustering on make_circles (eps={eps_circles}, min_samples={min_samples_circles})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"Zhy8ntswsw5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids."],"metadata":{"id":"_D6Z55uHsx4H"}},{"cell_type":"code","source":["print(\"--- Task 26: K-Means on Breast Cancer with MinMaxScaler, output centroids ---\")\n","# Load Breast Cancer dataset\n","breast_cancer = load_breast_cancer()\n","X_bc, y_bc = breast_cancer.data, breast_cancer.target\n","\n","# Apply MinMaxScaler\n","scaler_minmax = MinMaxScaler()\n","X_bc_scaled = scaler_minmax.fit_transform(X_bc)\n","\n","# Apply K-Means clustering (Breast Cancer has 2 classes)\n","n_clusters_bc = 2\n","kmeans_bc = KMeans(n_clusters=n_clusters_bc, random_state=42, n_init=10)\n","kmeans_bc.fit(X_bc_scaled)\n","labels_kmeans_bc = kmeans_bc.labels_\n","centroids_bc = kmeans_bc.cluster_centers_\n","\n","print(\"K-Means Cluster Centroids (after MinMaxScaler):\")\n","# You can map these centroids back to original scale if needed, but for now, in scaled space\n","for i, centroid in enumerate(centroids_bc):\n","    print(f\"Cluster {i} Centroid:\\n {centroid}\")\n","\n","# Optional: Check accuracy (since it's a known 2-class dataset)\n","# Note: K-Means labels are arbitrary (0 can be malignant or benign)\n","# You might need to remap labels to match true labels for accuracy calculation\n","from sklearn.metrics import adjusted_rand_score\n","ari = adjusted_rand_score(y_bc, labels_kmeans_bc)\n","print(f\"\\nAdjusted Rand Index (measures similarity of clusterings): {ari:.4f}\")\n","print(\"-\" * 50)"],"metadata":{"id":"DK2_Y4y4szJ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["27. Generate synthetic data with make_blobs with varying cluster standard deviations and cluster with DBSCAN."],"metadata":{"id":"m4ghVPNNs0E4"}},{"cell_type":"code","source":["print(\"--- Task 27: DBSCAN on make_blobs with varying std ---\")\n","# Generate synthetic data with make_blobs and varying cluster standard deviations\n","n_samples_std = 700\n","n_features_std = 2\n","n_centers_std = 3\n","cluster_std_values = [0.5, 1.5, 0.8] # Varying standard deviations\n","X_blobs_std, y_blobs_std = make_blobs(n_samples=n_samples_std, centers=n_centers_std,\n","                                     n_features=n_features_std, cluster_std=cluster_std_values,\n","                                     random_state=42)\n","\n","# Apply DBSCAN\n","# Tuning eps and min_samples is crucial here. Start with a visual inspection.\n","# Setting eps can be tricky for varying densities.\n","eps_std = 0.5 # A chosen epsilon\n","min_samples_std = 10 # A chosen min_samples\n","dbscan_std = DBSCAN(eps=eps_std, min_samples=min_samples_std)\n","labels_dbscan_std = dbscan_std.fit_predict(X_blobs_std)\n","\n","n_clusters_found = len(set(labels_dbscan_std)) - (1 if -1 in labels_dbscan_std else 0)\n","n_noise_found = np.sum(labels_dbscan_std == -1)\n","\n","print(f\"DBSCAN found {n_clusters_found} clusters and {n_noise_found} noise points.\")\n","\n","# Visualize the clusters and noise\n","plt.figure(figsize=(8, 6))\n","unique_labels_std = set(labels_dbscan_std)\n","colors_std = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_std))]\n","for k, col in zip(unique_labels_std, colors_std):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask = (labels_dbscan_std == k)\n","    xy = X_blobs_std[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","\n","plt.title(f'DBSCAN on make_blobs with Varying Std Dev (eps={eps_std}, min_samples={min_samples_std})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"1LyNXl6Ms4QK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means."],"metadata":{"id":"m5vuHJrus5uy"}},{"cell_type":"code","source":["print(\"--- Task 28: K-Means on Digits (PCA 2D) and Visualize ---\")\n","# Load Digits dataset\n","digits = load_digits()\n","X_digits, y_digits = digits.data, digits.target # 10 classes (0-9)\n","\n","# Reduce dimensionality to 2D using PCA\n","pca = PCA(n_components=2, random_state=42)\n","X_digits_pca = pca.fit_transform(X_digits)\n","\n","# Apply K-Means clustering (10 clusters for digits)\n","n_clusters_digits = 10\n","kmeans_digits = KMeans(n_clusters=n_clusters_digits, random_state=42, n_init=10)\n","kmeans_digits.fit(X_digits_pca)\n","labels_kmeans_digits = kmeans_digits.labels_\n","centroids_digits_pca = kmeans_digits.cluster_centers_\n","\n","# Visualize the clusters in 2D PCA space\n","plt.figure(figsize=(10, 8))\n","plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1], c=labels_kmeans_digits, cmap='tab10', s=40, alpha=0.8)\n","plt.scatter(centroids_digits_pca[:, 0], centroids_digits_pca[:, 1], c='red', marker='X', s=200, label='Centroids', edgecolors='black')\n","plt.title('K-Means Clustering on Digits Dataset (PCA to 2D)')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"WC48KkDYs7KQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["29. Create synthetic data using make_blobs and evaluate silhouette scores for k=2 to 5. Display as a bar chart."],"metadata":{"id":"rII0A3Wfs8Fs"}},{"cell_type":"code","source":["print(\"--- Task 29: Silhouette Scores for K-Means with make_blobs (k=2 to 5) ---\")\n","# Generate synthetic data\n","X_sil, y_sil = make_blobs(n_samples=500, centers=4, n_features=2, random_state=42)\n","\n","k_values = range(2, 6) # k = 2, 3, 4, 5\n","silhouette_scores_list = []\n","\n","for k in k_values:\n","    kmeans_sil = KMeans(n_clusters=k, random_state=42, n_init=10)\n","    kmeans_sil.fit(X_sil)\n","    score = silhouette_score(X_sil, kmeans_sil.labels_)\n","    silhouette_scores_list.append(score)\n","    print(f\"Silhouette Score for k={k}: {score:.4f}\")\n","\n","# Display as a bar chart\n","plt.figure(figsize=(8, 6))\n","plt.bar(k_values, silhouette_scores_list, color='skyblue')\n","plt.xlabel('Number of Clusters (k)')\n","plt.ylabel('Silhouette Score')\n","plt.title('Silhouette Scores for K-Means with different k values')\n","plt.xticks(k_values)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"eEo5M1pbs9Ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage."],"metadata":{"id":"uqArSl31s-PV"}},{"cell_type":"code","source":["print(\"--- Task 30: Hierarchical Clustering on Iris, Dendrogram with Average Linkage ---\")\n","from scipy.cluster.hierarchy import dendrogram, linkage # Import for dendrogram\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X_iris_dendro = iris.data\n","\n","# Perform hierarchical clustering using 'average' linkage\n","linked = linkage(X_iris_dendro, method='average')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(12, 8))\n","dendrogram(linked,\n","           orientation='top',\n","           labels=iris.target_names[iris.target], # Optional: label leaves with true class names\n","           distance_sort='descending',\n","           show_leaf_counts=True)\n","plt.title('Hierarchical Clustering Dendrogram (Iris Dataset, Average Linkage)')\n","plt.xlabel('Data Point Index (or Cluster Size)')\n","plt.ylabel('Distance')\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"A8JKm9mDs_lp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries."],"metadata":{"id":"sl4rgxTgtAee"}},{"cell_type":"code","source":["print(\"--- Task 31: K-Means on Overlapping Blobs, Visualize Decision Boundaries ---\")\n","# Generate synthetic data with overlapping clusters\n","# Increase cluster_std to create overlap\n","X_overlap, y_overlap = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=1.5, random_state=42)\n","\n","# Apply K-Means\n","n_clusters_overlap = 3\n","kmeans_overlap = KMeans(n_clusters=n_clusters_overlap, random_state=42, n_init=10)\n","kmeans_overlap.fit(X_overlap)\n","labels_overlap = kmeans_overlap.labels_\n","centroids_overlap = kmeans_overlap.cluster_centers_\n","\n","# Visualize with decision boundaries\n","plt.figure(figsize=(10, 8))\n","# Plot data points\n","plt.scatter(X_overlap[:, 0], X_overlap[:, 1], c=labels_overlap, cmap='viridis', s=50, alpha=0.8, edgecolors='k')\n","\n","# Plot centroids\n","plt.scatter(centroids_overlap[:, 0], centroids_overlap[:, 1], c='red', marker='X', s=200, label='Centroids', edgecolors='black')\n","\n","# Plot decision boundaries (Voronoi tessellation)\n","# Create meshgrid\n","x_min, x_max = X_overlap[:, 0].min() - 1, X_overlap[:, 0].max() + 1\n","y_min, y_max = X_overlap[:, 1].min() - 1, X_overlap[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n","                     np.arange(y_min, y_max, 0.02))\n","\n","# Predict cluster for each point in meshgrid\n","Z = kmeans_overlap.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n","# Plot contours\n","plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.3)\n","\n","plt.title('K-Means Clustering on Overlapping Blobs with Decision Boundaries')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"xPedQh4ytCBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results."],"metadata":{"id":"tgEatgkOtC5C"}},{"cell_type":"code","source":["print(\"--- Task 32: DBSCAN on Digits (t-SNE 2D) and Visualize ---\")\n","# Load Digits dataset\n","digits_tsne = load_digits()\n","X_digits_tsne, y_digits_tsne = digits_tsne.data, digits_tsne.target\n","\n","# Reduce dimensionality to 2D using t-SNE\n","# t-SNE is computationally intensive, can take a while for large datasets.\n","# For demonstration, we'll use a smaller number of samples if needed, or just let it run.\n","# For Digits (1797 samples), it's manageable.\n","tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000) # Default perplexity/n_iter are usually fine\n","X_digits_tsne_2d = tsne.fit_transform(X_digits_tsne)\n","\n","# Apply DBSCAN\n","# DBSCAN parameters (eps, min_samples) are crucial and need tuning for t-SNE output\n","# t-SNE preserves local structure but can distort distances, so eps choice is important.\n","eps_digits_tsne = 1.8 # Example value, needs tuning for optimal results\n","min_samples_digits_tsne = 10 # Example value, needs tuning\n","dbscan_digits_tsne = DBSCAN(eps=eps_digits_tsne, min_samples=min_samples_digits_tsne)\n","labels_dbscan_digits_tsne = dbscan_digits_tsne.fit_predict(X_digits_tsne_2d)\n","\n","n_clusters_tsne = len(set(labels_dbscan_digits_tsne)) - (1 if -1 in labels_dbscan_digits_tsne else 0)\n","n_noise_tsne = np.sum(labels_dbscan_digits_tsne == -1)\n","\n","print(f\"DBSCAN found {n_clusters_tsne} clusters and {n_noise_tsne} noise points after t-SNE reduction.\")\n","\n","# Visualize the clusters and noise in 2D t-SNE space\n","plt.figure(figsize=(10, 8))\n","unique_labels_tsne = set(labels_dbscan_digits_tsne)\n","colors_tsne = [plt.cm.jet(each) for each in np.linspace(0, 1, len(unique_labels_tsne))]\n","for k, col in zip(unique_labels_tsne, colors_tsne):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask = (labels_dbscan_digits_tsne == k)\n","    xy = X_digits_tsne_2d[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","\n","plt.title(f'DBSCAN Clustering on Digits Dataset (t-SNE to 2D, eps={eps_digits_tsne}, min_samples={min_samples_digits_tsne})')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"YkwbGS1vtEIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result."],"metadata":{"id":"CTfrJvzCtFTd"}},{"cell_type":"code","source":["print(\"--- Task 33: Agglomerative Clustering on make_blobs with Complete Linkage ---\")\n","# Generate synthetic data\n","X_agg_complete, y_agg_complete = make_blobs(n_samples=500, centers=4, n_features=2, random_state=42)\n","\n","# Apply Agglomerative Clustering with complete linkage\n","n_clusters_agg_complete = 4 # Assuming 4 clusters as per make_blobs centers\n","agg_complete = AgglomerativeClustering(n_clusters=n_clusters_agg_complete, linkage='complete')\n","labels_agg_complete = agg_complete.fit_predict(X_agg_complete)\n","\n","# Visualize the clusters\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_agg_complete[:, 0], X_agg_complete[:, 1], c=labels_agg_complete, cmap='viridis', s=50, alpha=0.8)\n","plt.title(f'Agglomerative Clustering (Complete Linkage) with {n_clusters_agg_complete} Clusters')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"-RYST464tGjc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["34. Load the Breast Cancer dataset and compare inertia values for k=2 to 6 using K-Means. Show results in a line plot."],"metadata":{"id":"Ep4hXkJItHsl"}},{"cell_type":"code","source":["print(\"--- Task 34: K-Means Inertia on Breast Cancer (k=2 to 6) ---\")\n","# Load Breast Cancer dataset (already loaded as X_bc, y_bc in Task 26)\n","# Standardize the data first for meaningful inertia comparison\n","scaler_bc_inertia = StandardScaler()\n","X_bc_scaled_inertia = scaler_bc_inertia.fit_transform(X_bc)\n","\n","k_values_inertia = range(2, 7) # k = 2, 3, 4, 5, 6\n","inertia_values = []\n","\n","for k in k_values_inertia:\n","    kmeans_inertia = KMeans(n_clusters=k, random_state=42, n_init=10)\n","    kmeans_inertia.fit(X_bc_scaled_inertia)\n","    inertia_values.append(kmeans_inertia.inertia_)\n","    print(f\"Inertia for k={k}: {kmeans_inertia.inertia_:.4f}\")\n","\n","# Plot the elbow curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(k_values_inertia, inertia_values, marker='o', linestyle='-')\n","plt.title('K-Means Inertia vs. Number of Clusters (Breast Cancer Dataset)')\n","plt.xlabel('Number of Clusters (k)')\n","plt.ylabel('Inertia (Within-cluster Sum of Squares)')\n","plt.xticks(k_values_inertia)\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"SSaBqDWJtJQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage."],"metadata":{"id":"7yIPAy_otKTG"}},{"cell_type":"code","source":["print(\"--- Task 35: Agglomerative Clustering (Single Linkage) on make_circles ---\")\n","# Generate synthetic concentric circles\n","X_circles_agg, y_circles_agg = make_circles(n_samples=500, noise=0.05, factor=0.5, random_state=42)\n","\n","# Apply Agglomerative Clustering with single linkage\n","# n_clusters=2 for the two circles\n","n_clusters_circles_agg = 2\n","agg_single = AgglomerativeClustering(n_clusters=n_clusters_circles_agg, linkage='single')\n","labels_agg_single = agg_single.fit_predict(X_circles_agg)\n","\n","# Visualize the clusters\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_circles_agg[:, 0], X_circles_agg[:, 1], c=labels_agg_single, cmap='viridis', s=50, alpha=0.8)\n","plt.title(f'Agglomerative Clustering (Single Linkage) on make_circles ({n_clusters_circles_agg} Clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"qWBAqr73tLvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)."],"metadata":{"id":"lfbgXPJCtMqz"}},{"cell_type":"code","source":["print(\"--- Task 36: DBSCAN on Wine (scaled), count clusters ---\")\n","# Load Wine dataset (already loaded as X_wine, y_wine)\n","# Scale the data first\n","scaler_wine_dbscan = StandardScaler()\n","X_wine_scaled_dbscan = scaler_wine_dbscan.fit_transform(X_wine)\n","\n","# Apply DBSCAN\n","# DBSCAN parameters (eps, min_samples) need careful tuning for real-world datasets\n","# These are example values and might need adjustment for optimal clustering\n","eps_wine_dbscan = 1.5 # Example epsilon\n","min_samples_wine_dbscan = 5 # Example min_samples\n","dbscan_wine = DBSCAN(eps=eps_wine_dbscan, min_samples=min_samples_wine_dbscan)\n","labels_dbscan_wine = dbscan_wine.fit_predict(X_wine_scaled_dbscan)\n","\n","# Count the number of clusters (excluding noise)\n","n_clusters_wine_dbscan = len(set(labels_dbscan_wine)) - (1 if -1 in labels_dbscan_wine else 0)\n","n_noise_wine_dbscan = np.sum(labels_dbscan_wine == -1)\n","\n","print(f\"DBSCAN on Wine dataset (scaled):\")\n","print(f\"Number of clusters found: {n_clusters_wine_dbscan}\")\n","print(f\"Number of noise points: {n_noise_wine_dbscan}\")\n","print(f\"Cluster labels: {np.unique(labels_dbscan_wine)}\")\n","print(\"-\" * 50)"],"metadata":{"id":"VyvA-g2utOC6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["37. Generate synthetic data with make_blobs and apply K-Means. Then count the cluster centers on top of the data points."],"metadata":{"id":"BlT-VxIXtPHE"}},{"cell_type":"code","source":["print(\"--- Task 37: K-Means on make_blobs, Plot Centers on Data ---\")\n","# Generate synthetic data with 3 centers\n","n_samples_plot = 500\n","n_features_plot = 2\n","n_centers_plot = 3\n","X_plot, y_plot = make_blobs(n_samples=n_samples_plot, centers=n_centers_plot, n_features=n_features_plot, random_state=42)\n","\n","# Apply K-Means clustering\n","kmeans_plot = KMeans(n_clusters=n_centers_plot, random_state=42, n_init=10)\n","kmeans_plot.fit(X_plot)\n","labels_plot = kmeans_plot.labels_\n","centroids_plot = kmeans_plot.cluster_centers_\n","\n","# Visualize the clusters and centroids\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_plot[:, 0], X_plot[:, 1], c=labels_plot, cmap='viridis', s=50, alpha=0.8, label='Data Points')\n","plt.scatter(centroids_plot[:, 0], centroids_plot[:, 1], c='red', marker='X', s=200, label='Centroids', edgecolor='black', linewidth=1.5)\n","plt.title(f'K-Means Clustering with {n_centers_plot} Centers (Data and Centroids)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"mhSrdHaAtQjB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise."],"metadata":{"id":"lcrv4V5itRek"}},{"cell_type":"code","source":["print(\"--- Task 38: DBSCAN on Iris, print noise count ---\")\n","# Load Iris dataset (already loaded as X_iris, y_iris_true)\n","# Standardize the data first for DBSCAN\n","scaler_iris_dbscan = StandardScaler()\n","X_iris_scaled_dbscan = scaler_iris_dbscan.fit_transform(X_iris)\n","\n","# Apply DBSCAN\n","# Tuning eps and min_samples is crucial for Iris\n","# These values are chosen based on common practice for Iris, but can vary\n","eps_iris_dbscan = 0.5 # Example epsilon\n","min_samples_iris_dbscan = 5 # Example min_samples\n","dbscan_iris = DBSCAN(eps=eps_iris_dbscan, min_samples=min_samples_iris_dbscan)\n","labels_dbscan_iris = dbscan_iris.fit_predict(X_iris_scaled_dbscan)\n","\n","# Count noise points\n","n_noise_iris = np.sum(labels_dbscan_iris == -1)\n","print(f\"Number of samples identified as noise in Iris dataset by DBSCAN: {n_noise_iris}\")\n","\n","# Optional: Print cluster counts\n","n_clusters_iris_dbscan = len(set(labels_dbscan_iris)) - (1 if -1 in labels_dbscan_iris else 0)\n","print(f\"Number of clusters found: {n_clusters_iris_dbscan}\")\n","print(f\"Cluster labels: {np.unique(labels_dbscan_iris)}\")\n","\n","# Optional: Visualize (similar to previous DBSCAN plots)\n","plt.figure(figsize=(8, 6))\n","unique_labels_iris_dbscan = set(labels_dbscan_iris)\n","colors_iris_dbscan = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_iris_dbscan))]\n","for k, col in zip(unique_labels_iris_dbscan, colors_iris_dbscan):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask = (labels_dbscan_iris == k)\n","    xy = X_iris_scaled_dbscan[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","plt.title(f'DBSCAN on Iris (scaled, eps={eps_iris_dbscan}, min_samples={min_samples_iris_dbscan})')\n","plt.xlabel('Scaled Feature 1')\n","plt.ylabel('Scaled Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"unvp3cb6tSnQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result."],"metadata":{"id":"aVzI5YtrtTj3"}},{"cell_type":"code","source":["print(\"--- Task 39: K-Means on make_moons (Non-linear) ---\")\n","# Generate synthetic non-linearly separable data with make_moons\n","X_moons_kmeans, y_moons_kmeans = make_moons(n_samples=500, noise=0.05, random_state=42)\n","\n","# Apply K-Means clustering (expecting 2 \"true\" clusters from make_moons)\n","n_clusters_moons_kmeans = 2\n","kmeans_moons = KMeans(n_clusters=n_clusters_moons_kmeans, random_state=42, n_init=10)\n","kmeans_moons.fit(X_moons_kmeans)\n","labels_moons_kmeans = kmeans_moons.labels_\n","centroids_moons_kmeans = kmeans_moons.cluster_centers_\n","\n","# Visualize the clustering result\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_moons_kmeans[:, 0], X_moons_kmeans[:, 1], c=labels_moons_kmeans, cmap='viridis', s=50, alpha=0.8)\n","plt.scatter(centroids_moons_kmeans[:, 0], centroids_moons_kmeans[:, 1], c='red', marker='X', s=200, label='Centroids')\n","plt.title('K-Means Clustering on make_moons (Non-linear Data)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"FsvtKFLDtU-2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["40. Load the Digits dataset, apply PCA to reduce to 3 components, then use K-Means and visualize with a 3D scatter plot."],"metadata":{"id":"NDGo4xlotXJh"}},{"cell_type":"code","source":["print(\"--- Task 40: K-Means on Digits (PCA 3D) and Visualize 3D ---\")\n","# Load Digits dataset (already loaded as X_digits, y_digits)\n","\n","# Reduce dimensionality to 3D using PCA\n","pca_3d = PCA(n_components=3, random_state=42)\n","X_digits_pca_3d = pca_3d.fit_transform(X_digits)\n","\n","# Apply K-Means clustering (10 clusters for digits)\n","n_clusters_digits_3d = 10\n","kmeans_digits_3d = KMeans(n_clusters=n_clusters_digits_3d, random_state=42, n_init=10)\n","kmeans_digits_3d.fit(X_digits_pca_3d)\n","labels_kmeans_digits_3d = kmeans_digits_3d.labels_\n","centroids_digits_3d_pca = kmeans_digits_3d.cluster_centers_\n","\n","# Visualize the clusters in 3D PCA space\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Scatter plot data points\n","ax.scatter(X_digits_pca_3d[:, 0], X_digits_pca_3d[:, 1], X_digits_pca_3d[:, 2],\n","           c=labels_kmeans_digits_3d, cmap='tab10', s=40, alpha=0.8)\n","\n","# Scatter plot centroids\n","ax.scatter(centroids_digits_3d_pca[:, 0], centroids_digits_3d_pca[:, 1], centroids_digits_3d_pca[:, 2],\n","           c='red', marker='X', s=200, label='Centroids', edgecolor='black')\n","\n","ax.set_title('K-Means Clustering on Digits Dataset (PCA to 3D)')\n","ax.set_xlabel('Principal Component 1')\n","ax.set_ylabel('Principal Component 2')\n","ax.set_zlabel('Principal Component 3')\n","ax.legend()\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"Laycl9Q2tXix"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering."],"metadata":{"id":"6pnADQbWuFOr"}},{"cell_type":"code","source":["print(\"--- Task 41: K-Means on make_blobs with Silhouette Score ---\")\n","# Generate synthetic data with 5 centers\n","n_samples_41 = 500\n","n_features_41 = 2\n","n_centers_41 = 5\n","X_blobs_41, y_blobs_41 = make_blobs(n_samples=n_samples_41, centers=n_centers_41, n_features=n_features_41, random_state=42)\n","\n","# Apply K-Means clustering\n","kmeans_41 = KMeans(n_clusters=n_centers_41, random_state=42, n_init=10)\n","kmeans_41.fit(X_blobs_41)\n","labels_kmeans_41 = kmeans_41.labels_\n","\n","# Evaluate using Silhouette Score\n","silhouette_avg_41 = silhouette_score(X_blobs_41, labels_kmeans_41)\n","print(f\"Silhouette Score for K-Means with {n_centers_41} clusters: {silhouette_avg_41:.4f}\")\n","\n","# Visualize the clusters (optional, but good for understanding)\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_blobs_41[:, 0], X_blobs_41[:, 1], c=labels_kmeans_41, cmap='viridis', s=50, alpha=0.8)\n","plt.scatter(kmeans_41.cluster_centers_[:, 0], kmeans_41.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n","plt.title(f'K-Means Clustering with {n_centers_41} Centers (Silhouette Score: {silhouette_avg_41:.2f})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"DvSOK00fuNm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and Apply Agglomerative Clustering. Visualize in 2D."],"metadata":{"id":"cfd4cGEbuOqZ"}},{"cell_type":"code","source":["print(\"--- Task 42: Agglomerative Clustering on Breast Cancer (PCA 2D) ---\")\n","# Load Breast Cancer dataset\n","breast_cancer = load_breast_cancer()\n","X_bc_42, y_bc_42 = breast_cancer.data, breast_cancer.target\n","\n","# Standardize the data before PCA\n","scaler_42 = StandardScaler()\n","X_bc_scaled_42 = scaler_42.fit_transform(X_bc_42)\n","\n","# Reduce dimensionality to 2D using PCA\n","pca_42 = PCA(n_components=2, random_state=42)\n","X_bc_pca_42 = pca_42.fit_transform(X_bc_scaled_42)\n","\n","# Apply Agglomerative Clustering (Breast Cancer has 2 true classes)\n","n_clusters_42 = 2\n","agg_clustering_42 = AgglomerativeClustering(n_clusters=n_clusters_42, linkage='ward')\n","labels_agg_42 = agg_clustering_42.fit_predict(X_bc_pca_42)\n","\n","# Visualize the clusters in 2D PCA space\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_bc_pca_42[:, 0], X_bc_pca_42[:, 1], c=labels_agg_42, cmap='viridis', s=50, alpha=0.8)\n","plt.title(f'Agglomerative Clustering on Breast Cancer (PCA to 2D) with {n_clusters_42} Clusters')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","\n","# Optional: Evaluate with Adjusted Rand Index\n","ari_42 = adjusted_rand_score(y_bc_42, labels_agg_42)\n","print(f\"Adjusted Rand Index: {ari_42:.4f}\")\n","print(\"-\" * 50)"],"metadata":{"id":"89x3JhXnuP88"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["43. Generate noisy circular data using make_circles and visualize clustering results from K-Means and DBSCAN side-by-side."],"metadata":{"id":"PDpzb62XuRN3"}},{"cell_type":"code","source":["print(\"--- Task 43: K-Means vs DBSCAN on Noisy Circles ---\")\n","# Generate noisy circular data\n","n_samples_43 = 500\n","X_circles_43, y_circles_43 = make_circles(n_samples=n_samples_43, noise=0.1, factor=0.5, random_state=42)\n","\n","# --- Apply K-Means ---\n","kmeans_43 = KMeans(n_clusters=2, random_state=42, n_init=10) # 2 true clusters\n","kmeans_43.fit(X_circles_43)\n","labels_kmeans_43 = kmeans_43.labels_\n","\n","# --- Apply DBSCAN ---\n","# DBSCAN parameters are crucial for this dataset.\n","# You might need to tune these for optimal results\n","eps_43 = 0.15 # Example value, needs tuning\n","min_samples_43 = 5 # Example value, needs tuning\n","dbscan_43 = DBSCAN(eps=eps_43, min_samples=min_samples_43)\n","labels_dbscan_43 = dbscan_43.fit_predict(X_circles_43)\n","\n","n_clusters_dbscan_43 = len(set(labels_dbscan_43)) - (1 if -1 in labels_dbscan_43 else 0)\n","n_noise_dbscan_43 = np.sum(labels_dbscan_43 == -1)\n","print(f\"DBSCAN found {n_clusters_dbscan_43} clusters and {n_noise_dbscan_43} noise points.\")\n","\n","\n","# --- Visualize side-by-side ---\n","plt.figure(figsize=(16, 6))\n","\n","# K-Means plot\n","plt.subplot(1, 2, 1)\n","plt.scatter(X_circles_43[:, 0], X_circles_43[:, 1], c=labels_kmeans_43, cmap='viridis', s=50, alpha=0.8)\n","plt.scatter(kmeans_43.cluster_centers_[:, 0], kmeans_43.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n","plt.title('K-Means Clustering on Noisy Circles (Assumes spherical clusters)')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","\n","# DBSCAN plot\n","plt.subplot(1, 2, 2)\n","unique_labels_dbscan_43 = set(labels_dbscan_43)\n","colors_dbscan_43 = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_dbscan_43))]\n","for k, col in zip(unique_labels_dbscan_43, colors_dbscan_43):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask = (labels_dbscan_43 == k)\n","    xy = X_circles_43[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","plt.title(f'DBSCAN Clustering on Noisy Circles (eps={eps_43}, min_samples={min_samples_43})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"nCkNFnFluaSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering."],"metadata":{"id":"yG9u2VOIua8x"}},{"cell_type":"code","source":["print(\"--- Task 44: Iris K-Means, Plot Silhouette Coefficient for each sample ---\")\n","from sklearn.metrics import silhouette_samples\n","\n","# Load Iris dataset (already loaded as X_iris, y_iris_true)\n","X_iris_44 = iris.data\n","\n","# Apply K-Means clustering (3 true clusters for Iris)\n","n_clusters_iris_44 = 3\n","kmeans_iris_44 = KMeans(n_clusters=n_clusters_iris_44, random_state=42, n_init=10)\n","kmeans_iris_44.fit(X_iris_44)\n","labels_kmeans_iris_44 = kmeans_iris_44.labels_\n","\n","# Calculate the silhouette score for each sample\n","sample_silhouette_values = silhouette_samples(X_iris_44, labels_kmeans_iris_44)\n","silhouette_avg_44 = silhouette_score(X_iris_44, labels_kmeans_iris_44)\n","\n","print(f\"Overall Silhouette Average for Iris K-Means: {silhouette_avg_44:.4f}\")\n","\n","# Plotting the silhouette diagram\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","y_lower = 10\n","for i in range(n_clusters_iris_44):\n","    # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n","    ith_cluster_silhouette_values = sample_silhouette_values[labels_kmeans_iris_44 == i]\n","    ith_cluster_silhouette_values.sort()\n","\n","    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = plt.cm.nipy_spectral(float(i) / n_clusters_iris_44)\n","    ax.fill_betweenx(np.arange(y_lower, y_upper),\n","                     0, ith_cluster_silhouette_values,\n","                     facecolor=color, edgecolor=color, alpha=0.7)\n","\n","    # Label the silhouette plots with their cluster numbers at the middle\n","    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","    # Compute the new y_lower for next plot\n","    y_lower = y_upper + 10 # 10 for the 0 samples\n","\n","ax.set_title(\"Silhouette analysis for KMeans clustering on Iris data\")\n","ax.set_xlabel(\"Silhouette coefficient values\")\n","ax.set_ylabel(\"Cluster label\")\n","\n","# The vertical line for average silhouette score of all the values\n","ax.axvline(x=silhouette_avg_44, color=\"red\", linestyle=\"--\", label=f'Average Silhouette Score: {silhouette_avg_44:.2f}')\n","ax.set_yticks([]) # Clear the yaxis labels / ticks\n","ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","ax.legend()\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"tMXmrzt5ucpc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with average linkage. Visualize clusters."],"metadata":{"id":"BEsA5q2RudrK"}},{"cell_type":"code","source":["print(\"--- Task 45: Agglomerative Clustering on make_blobs with Average Linkage ---\")\n","# Generate synthetic data\n","X_agg_avg_45, y_agg_avg_45 = make_blobs(n_samples=500, centers=4, n_features=2, random_state=42)\n","\n","# Apply Agglomerative Clustering with average linkage\n","n_clusters_agg_avg_45 = 4\n","agg_average = AgglomerativeClustering(n_clusters=n_clusters_agg_avg_45, linkage='average')\n","labels_agg_avg_45 = agg_average.fit_predict(X_agg_avg_45)\n","\n","# Visualize the clusters\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_agg_avg_45[:, 0], X_agg_avg_45[:, 1], c=labels_agg_avg_45, cmap='viridis', s=50, alpha=0.8)\n","plt.title(f'Agglomerative Clustering (Average Linkage) with {n_clusters_agg_avg_45} Clusters')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"xnO9AbNuufEA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)."],"metadata":{"id":"hO81IWsCugEE"}},{"cell_type":"code","source":["print(\"--- Task 46: K-Means on Wine, Pairplot Visualization ---\")\n","# Load Wine dataset\n","wine = load_wine()\n","X_wine_46, y_wine_46 = wine.data, wine.target\n","feature_names_wine = wine.feature_names\n","\n","# Standardize the data\n","scaler_46 = StandardScaler()\n","X_wine_scaled_46 = scaler_46.fit_transform(X_wine_46)\n","\n","# Apply K-Means (Wine has 3 true classes)\n","n_clusters_wine_46 = 3\n","kmeans_wine_46 = KMeans(n_clusters=n_clusters_wine_46, random_state=42, n_init=10)\n","kmeans_wine_46.fit(X_wine_scaled_46)\n","labels_kmeans_wine_46 = kmeans_wine_46.labels_\n","\n","# Create a DataFrame for pairplot (using first 4 features as requested)\n","df_wine_46 = pd.DataFrame(X_wine_scaled_46[:, :4], columns=feature_names_wine[:4])\n","df_wine_46['Cluster'] = labels_kmeans_wine_46\n","df_wine_46['True Label'] = y_wine_46 # For comparison if needed\n","\n","# Visualize using seaborn pairplot\n","# Hue 'Cluster' to color by predicted cluster\n","# You can compare with 'True Label' by changing hue to 'True Label'\n","sns.pairplot(df_wine_46, hue='Cluster', palette='viridis', diag_kind='kde')\n","plt.suptitle('K-Means Cluster Assignments on Wine Dataset (First 4 Features)', y=1.02) # Adjust title position\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"EbakoLtcuis_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count."],"metadata":{"id":"Gci6qo03ujz7"}},{"cell_type":"code","source":["print(\"--- Task 47: DBSCAN on Noisy Blobs, Count Clusters and Noise ---\")\n","# Generate noisy blobs\n","n_samples_47 = 700\n","n_features_47 = 2\n","n_centers_47 = 3\n","cluster_std_47 = 1.0 # Slightly increased std for some noise\n","X_blobs_47, y_blobs_47 = make_blobs(n_samples=n_samples_47, centers=n_centers_47, n_features=n_features_47, cluster_std=cluster_std_47, random_state=42)\n","\n","# Introduce some artificial noise (optional, make_blobs noise is usually minor)\n","# np.random.seed(0)\n","# X_blobs_47 = np.vstack([X_blobs_47, np.random.uniform(low=-10, high=10, size=(50, 2))])\n","\n","# Apply DBSCAN\n","# Tuning eps and min_samples is crucial for DBSCAN\n","eps_47 = 0.8 # Example value, needs tuning\n","min_samples_47 = 10 # Example value, needs tuning\n","dbscan_47 = DBSCAN(eps=eps_47, min_samples=min_samples_47)\n","labels_dbscan_47 = dbscan_47.fit_predict(X_blobs_47)\n","\n","# Count the number of clusters (excluding noise)\n","n_clusters_47 = len(set(labels_dbscan_47)) - (1 if -1 in labels_dbscan_47 else 0)\n","n_noise_points_47 = np.sum(labels_dbscan_47 == -1)\n","\n","print(f\"DBSCAN found {n_clusters_47} clusters and {n_noise_points_47} noise points.\")\n","print(f\"Cluster labels found by DBSCAN: {np.unique(labels_dbscan_47)}\")\n","\n","# Visualize the clusters and noise\n","plt.figure(figsize=(8, 6))\n","unique_labels_47 = set(labels_dbscan_47)\n","colors_47 = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels_47))]\n","for k, col in zip(unique_labels_47, colors_47):\n","    if k == -1:\n","        col = [0, 0, 0, 1] # Black for noise\n","    class_member_mask = (labels_dbscan_47 == k)\n","    xy = X_blobs_47[class_member_mask]\n","    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n","             markeredgecolor='k', markersize=8 if k != -1 else 6,\n","             label=f'Cluster {k}' if k != -1 else 'Noise')\n","plt.title(f'DBSCAN on Noisy Blobs (eps={eps_47}, min_samples={min_samples_47})')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","print(\"-\" * 50)"],"metadata":{"id":"qDpweDJZulfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters."],"metadata":{"id":"EVF8LE0QuptO"}},{"cell_type":"code","source":["print(\"--- Task 48: Agglomerative Clustering on Digits (t-SNE 2D) ---\")\n","# Load Digits dataset\n","digits_48 = load_digits()\n","X_digits_48, y_digits_48 = digits_48.data, digits_48.target\n","\n","# Reduce dimensionality to 2D using t-SNE\n","# t-SNE is computationally intensive, consider its parameters carefully for larger datasets.\n","tsne_48 = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n","X_digits_tsne_48 = tsne_48.fit_transform(X_digits_48)\n","\n","# Apply Agglomerative Clustering (10 true clusters for digits)\n","n_clusters_agg_48 = 10 # Assuming 10 clusters for digits\n","agg_clustering_48 = AgglomerativeClustering(n_clusters=n_clusters_agg_48, linkage='ward') # 'ward' is good for general use\n","labels_agg_48 = agg_clustering_48.fit_predict(X_digits_tsne_48)\n","\n","# Visualize the clusters in 2D t-SNE space\n","plt.figure(figsize=(10, 8))\n","plt.scatter(X_digits_tsne_48[:, 0], X_digits_tsne_48[:, 1], c=labels_agg_48, cmap='tab10', s=40, alpha=0.8)\n","plt.title(f'Agglomerative Clustering on Digits Dataset (t-SNE to 2D) with {n_clusters_agg_48} Clusters')\n","plt.xlabel('t-SNE Component 1')\n","plt.ylabel('t-SNE Component 2')\n","plt.colorbar(label='Cluster Label')\n","plt.grid(True)\n","plt.show()\n","\n","# Optional: Evaluate with Adjusted Rand Index\n","ari_48 = adjusted_rand_score(y_digits_48, labels_agg_48)\n","print(f\"Adjusted Rand Index: {ari_48:.4f}\")\n","print(\"-\" * 50)"],"metadata":{"id":"a0g5Wgh-urQV"},"execution_count":null,"outputs":[]}]}